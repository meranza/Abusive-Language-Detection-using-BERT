# ðŸš¨ Abusive Language Detection using BERT

This project detects abusive comments on social media using **Natural Language Processing (NLP)** and a **BERT-based deep learning model**. It leverages pre-trained transformer embeddings to classify text into multiple categories of abuse with high accuracy.

---

## ðŸ§  Model Overview

- **Model**: DistilBERT (`distilbert-base-multilingual-cased`)
- **Framework**: PyTorch + HuggingFace Transformers
- **Dataset**: 143,000+ labeled comments (Kaggle Toxic Comment Classification)
- **Output Labels**:
  - `toxic`
  - `severe_toxic`
  - `obscene`
  - `threat`
  - `insult`
  - `identity_hate`

---
## ðŸ“¥ Dataset

- **Source**: [Kaggle Jigsaw Multilingual Toxic Comment Classification Challenge](https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification)

---
## ðŸ”§ Tech Stack

- Python
- PyTorch
- HuggingFace Transformers
- Flask (for web deployment)
- HTML/CSS (frontend)

---
## ðŸš€ How to Run

1. **Install dependencies**
   ```bash
   pip install flask torch transformers

2. **Run the Flask app**
   ```bash
   python app.py
---
## ðŸ–¼ Example

Here is how the app looks when running locally:

![Abusive Language Detection Example](Example.png)

## ðŸ“Œ Future Work

- Add BiLSTM comparison  
- Host live version online (Render, Replit, Hugging Face Spaces)  
- Improve UX with JavaScript and loading spinner

---
## ðŸ“‚ Suggested Folder Structure
abusive-language-detection/
â”‚
â”œâ”€â”€ app.py                  # Flask backend using BERT
â”œâ”€â”€ model.pt                # Trained PyTorch model
â”œâ”€â”€ model.py                # Model architecture (DistilBERT + classifier)
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html          # User input form
â”‚   â””â”€â”€ result.html         # Classification result display
â”œâ”€â”€ README.md
